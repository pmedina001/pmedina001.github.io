# -*- coding: utf-8 -*-
"""AI Models Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YWWwuN4NMT2eOdKcwcDB0FDuTxEJbWDv
"""

# Feature importance from best-performing model per crime type (clean format)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Load dataset
file_path = "/content/Combined_Crime_SocioEconomic_Data.xlsx"
df = pd.read_excel(file_path)
df["Population"] = df["Population"].replace(0, 1)

# Output size of combined dataset
print(f"Combined dataset shape: {df.shape[0]} rows, {df.shape[1]} columns")

# Normalize crime counts to rates per 100,000
crime_columns = [
    "Murder", "Rape", "Robbery", "Aggravated Assault",
    "Burglary", "Larceny", "Motor Vehicle Theft"
]
for col in crime_columns:
    df[col + " Rate"] = (df[col] / df["Population"]) * 100000

# Define features based on correlation analysis results
features = [
    "Population Density", "Housing Units", "Employment Rates",
    "Educational Attainment Levels (Bachelors Degree or Higher)", "Poverty Rates"
]

# Define targets (crime rate columns)
crime_targets = [col + " Rate" for col in crime_columns]

# Create output directories
output_dir = "/content/crime_model_plots"
importance_dir = "/content/feature_importance_plots"
best_model_plot_dir = "/content/best_model_actual_vs_predicted"
os.makedirs(output_dir, exist_ok=True)
os.makedirs(importance_dir, exist_ok=True)
os.makedirs(best_model_plot_dir, exist_ok=True)

# Initialize results list
results = []

# Loop through each crime type target
for target in crime_targets:
    df_model = df.dropna(subset=[target] + features)
    X = df_model[features]
    y = df_model[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model_results = {}

    # Random Forest
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    model_results["Random Forest"] = {
        "model": rf,
        "rmse": np.sqrt(mean_squared_error(y_test, y_pred_rf)),
        "r2": r2_score(y_test, y_pred_rf),
        "preds": y_pred_rf
    }

    # XGBoost
    xgb = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.2, random_state=42)
    xgb.fit(X_train, y_train)
    y_pred_xgb = xgb.predict(X_test)
    model_results["XGBoost"] = {
        "model": xgb,
        "rmse": np.sqrt(mean_squared_error(y_test, y_pred_xgb)),
        "r2": r2_score(y_test, y_pred_xgb),
        "preds": y_pred_xgb
    }

    # Decision Tree
    dt = DecisionTreeRegressor(random_state=42)
    dt.fit(X_train, y_train)
    y_pred_dt = dt.predict(X_test)
    model_results["Decision Tree"] = {
        "model": dt,
        "rmse": np.sqrt(mean_squared_error(y_test, y_pred_dt)),
        "r2": r2_score(y_test, y_pred_dt),
        "preds": y_pred_dt
    }

    # Select best model based on R²
    best_model_name = max(model_results, key=lambda k: model_results[k]["r2"])
    best_model_info = model_results[best_model_name]
    results.append([target, best_model_name, best_model_info["rmse"], best_model_info["r2"]])

    # Plot clean feature importance from best model
    plt.figure(figsize=(6, 4))
    if best_model_name in ["Random Forest", "Decision Tree"]:
        importances = pd.Series(best_model_info["model"].feature_importances_, index=features).sort_values()
        importances.plot(kind='barh', color='steelblue')
        plt.title(f"{target} - {best_model_name} Feature Importance")
        plt.xlabel("Importance")
        plt.tight_layout()
    elif best_model_name == "XGBoost":
        importances = pd.Series(best_model_info["model"].feature_importances_, index=features).sort_values()
        importances.plot(kind='barh', color='steelblue')
        plt.title(f"{target} - XGBoost Feature Importance")
        plt.xlabel("Importance")
        plt.tight_layout()

    plt.savefig(os.path.join(importance_dir, f"{target.replace(' ', '_')}_{best_model_name.lower()}_importance.png"))
    plt.close()

    # --- Plot Actual vs Predicted (Best Model Only) ---
    plt.figure(figsize=(6, 5))
    sns.scatterplot(x=y_test, y=best_model_info["preds"])
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.title(f"{target} - {best_model_name}")
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.tight_layout()
    best_plot_filename = os.path.join(best_model_plot_dir, f"{target.replace(' ', '_')}_{best_model_name.replace(' ', '_')}_actual_vs_predicted.png")
    plt.savefig(best_plot_filename)
    plt.close()

# Convert results to DataFrame and print
results_df = pd.DataFrame(results, columns=["Crime Type", "Best Model", "RMSE", "R² Score"])
print(results_df)

# --- Final Exportable Feature Importance Plots (Clean Layout) ---

# Create final output directory
final_output_dir = "/mnt/data/final_feature_importance_charts"
os.makedirs(final_output_dir, exist_ok=True)

# Define updated features again for consistency
final_features = [
    "Population Density",
    "Housing Units",
    "Employment Rates",
    "Educational Attainment Levels (Bachelors Degree or Higher)",
    "Poverty Rates"
]

# Filter the best models already evaluated and stored in 'results'
for crime, model_name, _, _ in results:
    model_row = next((r for r in results if r[0] == crime and r[1] == model_name), None)
    if not model_row:
        continue

    crime_clean = crime.replace(" Rate", "")
    model_obj = None
    df_model = df.dropna(subset=[crime] + final_features)
    X = df_model[final_features]
    y = df_model[crime]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    if model_name == "Random Forest":
        model_obj = RandomForestRegressor(n_estimators=100, random_state=42)
    elif model_name == "XGBoost":
        model_obj = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.2, random_state=42)
    elif model_name == "Decision Tree":
        model_obj = DecisionTreeRegressor(random_state=42)

    # Train the selected best model
    model_obj.fit(X_train, y_train)

    # Get feature importances
    if model_name == "XGBoost":
        importances = pd.Series(model_obj.feature_importances_, index=final_features).sort_values()
        xlabel = "Feature Importance"
    else:
        importances = pd.Series(model_obj.feature_importances_, index=final_features).sort_values()
        xlabel = "Feature Importance"

    # Plot clean, larger feature importance chart for presentation
    plt.figure(figsize=(12, 7))  # Wider + taller plot

    # Bar chart
    importances.plot(kind="barh", color="steelblue", edgecolor="black")

    # Title & labels (make sure nothing is cut off)
    plt.title(f"{target} - {best_model_name} Feature Importance", fontsize=18, pad=15)
    plt.xlabel("Importance", fontsize=14)
    plt.ylabel("")  # No need for ylabel, features are already on the y-axis

    # Ticks styling
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    # Remove grid
    plt.grid(False)

    # Adjust layout to prevent cutoff
    plt.tight_layout(pad=3.0)

    # Save the figure
    plt.savefig(os.path.join(importance_dir, f"{target.replace(' ', '_')}_{best_model_name.lower()}_importance.png"), dpi=300)
    plt.close()